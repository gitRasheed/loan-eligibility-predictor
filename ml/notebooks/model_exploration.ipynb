{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import optuna\n",
    "\n",
    "# Load the dataset\n",
    "loan_original = pd.read_csv('../data/loan_approval_dataset.csv', index_col='loan_id')\n",
    "\n",
    "loan_original.columns = loan_original.columns.str.strip()\n",
    "loan_original = loan_original.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Convert loan_status to binary\n",
    "loan_original['loan_status'] = loan_original['loan_status'].map({'Approved': 1, 'Rejected': 0})\n",
    "\n",
    "# Data Exploration\n",
    "print(loan_original.head())\n",
    "print(loan_original.info())\n",
    "print(loan_original.describe(include='all'))\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values:\")\n",
    "print(loan_original.isnull().sum())\n",
    "\n",
    "# Analyze loan status distribution\n",
    "print(\"\\nLoan Status Distribution:\")\n",
    "print(loan_original['loan_status'].value_counts(normalize=True))\n",
    "\n",
    "# Data Preparation and Feature Engineering\n",
    "# Convert remaining categorical variables to numeric\n",
    "loan_original['education'] = loan_original['education'].map({'Graduate': 1, 'NotGraduate': 0})\n",
    "loan_original['self_employed'] = loan_original['self_employed'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Log transform for highly skewed numeric columns\n",
    "numeric_columns = ['income_annum', 'loan_amount', 'residential_assets_value', \n",
    "                   'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']\n",
    "for col in numeric_columns:\n",
    "    loan_original[f'{col}_log'] = np.log1p(loan_original[col])\n",
    "\n",
    "# Feature engineering\n",
    "loan_original['loan_to_income_ratio'] = loan_original['loan_amount'] / loan_original['income_annum']\n",
    "loan_original['emi'] = loan_original['loan_amount'] / (loan_original['loan_term'] * 12)\n",
    "loan_original['total_assets'] = (loan_original['residential_assets_value'] + \n",
    "                                 loan_original['commercial_assets_value'] + \n",
    "                                 loan_original['luxury_assets_value'] + \n",
    "                                 loan_original['bank_asset_value'])\n",
    "loan_original['total_assets_log'] = np.log1p(loan_original['total_assets'])\n",
    "loan_original['loan_to_assets_ratio'] = loan_original['loan_amount'] / loan_original['total_assets']\n",
    "loan_original['income_per_dependent'] = loan_original['income_annum'] / (loan_original['no_of_dependents'] + 1)\n",
    "loan_original['balance_income'] = loan_original['income_annum'] - (loan_original['emi'] * 12)\n",
    "\n",
    "# Replace infinity values with NaN and then fill NaN\n",
    "loan_original = loan_original.replace([np.inf, -np.inf], np.nan)\n",
    "loan_original = loan_original.fillna(loan_original.median())\n",
    "\n",
    "# Model Exploration\n",
    "X = loan_original.drop('loan_status', axis=1)\n",
    "y = loan_original['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "def objective_logistic(trial):\n",
    "    model_params = {\n",
    "        'C': trial.suggest_float('C', 1e-5, 1e5, log=True),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "        'max_iter': trial.suggest_int('max_iter', 1000, 5000),\n",
    "        'tol': trial.suggest_float('tol', 1e-5, 1e-3, log=True)\n",
    "    }\n",
    "    threshold = trial.suggest_float('feature_selection_threshold', 0.01, 0.5)\n",
    "    \n",
    "    model = LogisticRegression(**model_params)\n",
    "    selector = SelectFromModel(estimator=model, threshold=threshold)\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    if selector.get_support().sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    X_selected = selector.transform(X_train_scaled)\n",
    "    return evaluate_model(model, X_selected, y_train)\n",
    "\n",
    "def objective_random_forest(trial):\n",
    "    model_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    }\n",
    "    threshold = trial.suggest_float('feature_selection_threshold', 0.01, 0.5)\n",
    "    \n",
    "    model = RandomForestClassifier(**model_params)\n",
    "    selector = SelectFromModel(estimator=model, threshold=threshold)\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    if selector.get_support().sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    X_selected = selector.transform(X_train_scaled)\n",
    "    return evaluate_model(model, X_selected, y_train)\n",
    "\n",
    "def objective_xgboost(trial):\n",
    "    model_params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    threshold = trial.suggest_float('feature_selection_threshold', 0.01, 0.5)\n",
    "    \n",
    "    model = XGBClassifier(**model_params)\n",
    "    selector = SelectFromModel(estimator=model, threshold=threshold)\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    if selector.get_support().sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    X_selected = selector.transform(X_train_scaled)\n",
    "    return evaluate_model(model, X_selected, y_train)\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    if X.shape[1] == 0:\n",
    "        return 0\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    except ValueError:\n",
    "        return 0\n",
    "# Run separate studies\n",
    "study_logistic = optuna.create_study(direction='maximize')\n",
    "study_logistic.optimize(objective_logistic, n_trials=30)\n",
    "\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_random_forest, n_trials=30)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgboost, n_trials=30)\n",
    "def plot_feature_importances(model, feature_names, title):\n",
    "    if hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(title)\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Get best models and selected features\n",
    "best_logistic_params = {k: v for k, v in study_logistic.best_params.items() if k != 'feature_selection_threshold'}\n",
    "best_logistic = LogisticRegression(**best_logistic_params)\n",
    "selector_logistic = SelectFromModel(best_logistic, threshold=study_logistic.best_params['feature_selection_threshold'])\n",
    "X_train_logistic = selector_logistic.fit_transform(X_train_scaled, y_train)\n",
    "X_test_logistic = selector_logistic.transform(X_test_scaled)\n",
    "\n",
    "best_rf_params = {k: v for k, v in study_rf.best_params.items() if k != 'feature_selection_threshold'}\n",
    "best_rf = RandomForestClassifier(**best_rf_params)\n",
    "selector_rf = SelectFromModel(best_rf, threshold=study_rf.best_params['feature_selection_threshold'])\n",
    "X_train_rf = selector_rf.fit_transform(X_train_scaled, y_train)\n",
    "X_test_rf = selector_rf.transform(X_test_scaled)\n",
    "\n",
    "best_xgb_params = {k: v for k, v in study_xgb.best_params.items() if k != 'feature_selection_threshold'}\n",
    "best_xgb = XGBClassifier(**best_xgb_params)\n",
    "selector_xgb = SelectFromModel(best_xgb, threshold=study_xgb.best_params['feature_selection_threshold'])\n",
    "X_train_xgb = selector_xgb.fit_transform(X_train_scaled, y_train)\n",
    "X_test_xgb = selector_xgb.transform(X_test_scaled)\n",
    "\n",
    "# Train and evaluate best models\n",
    "models = {\n",
    "    'Logistic Regression': (best_logistic, X_train_logistic, X_test_logistic, selector_logistic),\n",
    "    'Random Forest': (best_rf, X_train_rf, X_test_rf, selector_rf),\n",
    "    'XGBoost': (best_xgb, X_train_xgb, X_test_xgb, selector_xgb)\n",
    "}\n",
    "\n",
    "for name, (model, X_train_selected, X_test_selected, selector) in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    print(f\"\\nBest {name}:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    plot_confusion_matrix(y_test, y_pred, f'Confusion Matrix - {name}')\n",
    "    plot_feature_importances(model, X.columns[selector.get_support()], f\"Feature Importances - {name}\")\n",
    "\n",
    "# Visualization of optimization process\n",
    "def plot_optimization_history(study, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Extract trial numbers and corresponding values\n",
    "    trials = study.trials\n",
    "    values = [t.value for t in trials if t.value is not None]\n",
    "    trial_numbers = list(range(len(values)))\n",
    "    \n",
    "    # Plot the optimization history\n",
    "    plt.plot(trial_numbers, values, marker='o')\n",
    "    \n",
    "    # Plot the best value as a horizontal line\n",
    "    best_value = study.best_value\n",
    "    plt.axhline(y=best_value, color='r', linestyle='--', label='Best value')\n",
    "    \n",
    "    plt.xlabel('Trial number')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_overall_feature_importance(models, X):\n",
    "    overall_importance = np.zeros(X.shape[1])\n",
    "    feature_names = X.columns\n",
    "\n",
    "    for name, (model, _, _, selector) in models.items():\n",
    "        if hasattr(model, 'coef_'):\n",
    "            importances = np.abs(model.coef_[0])\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Normalize importances\n",
    "        importances = importances / np.sum(importances)\n",
    "        \n",
    "        # Add to overall importance\n",
    "        overall_importance[selector.get_support()] += importances\n",
    "\n",
    "    # Normalize overall importance\n",
    "    overall_importance = overall_importance / len(models)\n",
    "\n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(overall_importance)[::-1]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Overall Feature Importance Across All Models\")\n",
    "    plt.bar(range(len(overall_importance)), overall_importance[indices])\n",
    "    plt.xticks(range(len(overall_importance)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Plot optimization history and overall feature importance\n",
    "plot_optimization_history(study_logistic, \"Optimization History - Logistic Regression\")\n",
    "plot_optimization_history(study_rf, \"Optimization History - Random Forest\")\n",
    "plot_optimization_history(study_xgb, \"Optimization History - XGBoost\")\n",
    "plot_overall_feature_importance(models, X)\n",
    "\n",
    "def print_best_params_and_features(name, study, selector, X):\n",
    "    print(f\"\\nBest parameters for {name}:\")\n",
    "    params = study.best_params\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"\\nBest features for {name}:\")\n",
    "    for feature in selected_features:\n",
    "        print(f\"  {feature}\")\n",
    "\n",
    "# Best params and features for each model\n",
    "for name, study, selector in zip(['Logistic Regression', 'Random Forest', 'XGBoost'], \n",
    "                                 [study_logistic, study_rf, study_xgb],\n",
    "                                 [selector_logistic, selector_rf, selector_xgb]):\n",
    "    print_best_params_and_features(name, study, selector, X)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
